基于 BERT 的文本编码与相似度计算技术方案
1. 方案概述
本方案旨在利用 Google 开发的预训练模型 BERT (Bidirectional Encoder Representations from Transformers) 将文本转换为高维向量（Embedding），进而通过计算向量之间的数学距离来衡量文本的语义相似度。
适用场景：文本检索、问答系统匹配、查重、句子相似度打分。
2. 技术选型与架构
• 编程语言：Python 3.8+
• 深度学习框架：PyTorch (推荐) 或 TensorFlow
• NLP 工具库：Hugging Face Transformers (模型加载), Scikit-learn (相似度计算)
• 核心模型：bert-base-chinese (中文场景) 或 bert-base-uncased (英文场景)
3. 核心技术流程
使用 BERT 进行相似度计算主要分为四个阶段：数据预处理、模型推理、向量提取、相似度计算。
3.1 数据预处理
BERT 模型无法直接理解原始文本，需要通过 Tokenizer 将其转换为模型可接受的输入格式。
• 分词：将句子切分为字或词。
• 添加特殊标记：
    ◦ [CLS]：添加到句子开头，其对应的输出向量通常用于分类任务或全句语义表示。
    ◦ [SEP]：添加到句子结尾，用于分隔段落（单句情况下仅在末尾添加）。
• Padding & Truncation：将序列统一长度（BERT 通常限制为 512），不足处补 [PAD]，超过处截断。
3.2 模型推理
将处理好的 Input IDs 和 Attention Mask 输入 BERT 模型。
• Input IDs：Token 对应的数字索引。
• Attention Mask：区分哪些是真实 Token (1)，哪些是填充 Token (0)。
3.3 向量提取
BERT 输出包含每一层的隐藏状态。为了得到一个句子的向量表示，通常有三种策略：
1. 使用 [CLS] 向量（最简单）：取序列第一个 Token ([CLS]) 在最后一层的隐藏状态作为整句向量。
2. 平均池化（推荐）：计算最后一层所有 Token（排除 Padding）的向量平均值。
3. 最大池化：取每一维度的最大值。
    注：针对纯相似度任务，平均池化通常比 [CLS] 效果更稳定。
3.4 相似度计算
得到两个文本的向量 AA 和 BB 后，计算其余弦相似度：
Similarity=cos⁡(θ)=A⋅B∥A∥∥B∥Similarity=cos(θ)=∥A∥∥B∥A⋅B

结果范围在 [-1, 1] 之间，越接近 1 表示越相似。

graph TD
    A[输入: 文本1, 文本2] --> B[数据预处理模块]
    
    subgraph B [步骤 1: 预处理]
        B1[Tokenizer 分词]
        B2[添加 CLS/SEP 标记]
        B3[统一长度]
    end
    
    B --> C{BERT 模型层}
    C -->|Input IDs & Attention Mask| D[步骤 2: 模型编码]
    
    D --> E[输出: Hidden States]
    
    E --> F[步骤 3: 向量提取策略]
    
    subgraph F [提取策略]
        direction LR
        F1[方案 A: 取 CLS 向量]
        F2[方案 B: 平均池化 Mean Pooling]
    end
    
    F1 --> G[获取文本向量 V1]
    F2 --> G
    
    G --> H[步骤 4: 相似度计算]
    H -->|Cosine Similarity| I[输出: 相似度得分]

