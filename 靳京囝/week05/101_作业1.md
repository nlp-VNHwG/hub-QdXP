# 客服工作台自研方案分析（后端+算法开发角色）

## 一、需要设计数据库吗？
> 需要进行设计
- 业务实体需要“持久化”存储
- 业务规则需要“数据关系”来保障
- 双环境（测试/正式）必须有状态隔离
- 算法检索需要“结构化映射”
- 企业级应用必须可追溯、可审计

## 二、需要使用什么模型？

> **Sentence-BERT**
```
# 轻量级（CPU友好，响应快）
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # 12层，多语言

# 均衡型（精度/速度平衡）
model = SentenceTransformer('uer/sbert-base-chinese-nli')  # 中文专用

# 高精度（GPU推荐）
model = SentenceTransformer('BAAI/bge-small-zh')  # 中文检索SOTA
```
## 三、如何使用使用bert的？
> 第一阶段：模型准备
- 抛弃“拼接式”用法
- 采用Sentence-BERT架构
- 选择中文友好的预训练模型
- 确认向量维度

> 第二阶段：离线编码——让BERT阅读所有FAQ
- 从数据库读取待编码文本
- 文本清洗（适度即可）
- 批量送入BERT编码
- 执行Mean Pooling（均值池化）
- L2归一化
- 建立Faiss向量索引
- 持久化保存

> 第三阶段：在线检索——让BERT听懂用户
- 接收用户问题
- 文本清洗（与离线阶段完全一致）
- 实时编码
- 向量检索
- 阈值过滤
- 映射回业务数据
- 数据库补全信息
- 组装返回结果
> 第四阶段：维护与优化——让BERT越用越聪明
- 增量更新
- 失效处理
- 冷启动优化
- 效果评估
- 模型微调
- 降级兜底


## 四、是否需要使用大模型？

> **回答：非必须**

>1.任务本质

我们是标准问答匹配，不是开放生成。用户要的是“FAQ库里写死的答案”，不是大模型现编的答案。

>2.成本

大模型贵：GPU成本、延迟成本、运维成本。SBERT+Faiss单机CPU扛得住，大模型必须上GPU集群。

>2.确定性

大模型会“幻觉”——今天说退款要3天，明天说5天，客户投诉。FAQ答案是固定的，不允许模型乱改。

>4.响应速度

SBERT+Faiss <50ms，大模型（即使是6B量化） >500ms。客服场景要求秒级回复。


> 以下情况可考虑引入大模型
- 答案不是固定的，需要实时生成
- 用户问法远超200条相似问覆盖
- 需要多轮对话、追问、澄清