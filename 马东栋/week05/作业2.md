# 技术方案
#### 核心是利用BERT模型的上下文语义编码能力，将用户提问与历史FAQ提问转化为语义向量，通过向量相似度计算实现精准匹配，适配FAQ智能客服场景：
- 第一步，数据预处理。
  收集历史FAQ中的标准提问、相似提问及用户历史提问数据，进行清洗、标准化，
  同时结合FAQ生效时间，筛选出当前生效的提问数据，构建标准化提问语料库，作为相似度匹配的基础。

- 第二步，BERT文本编码。选用轻量级BERT预训练模型，
  无需微调即可满足基础语义匹配需求，若需提升准确率可基于自有FAQ语料微调。
  将预处理后的用户实时提问、历史FAQ提问，分别输入BERT模型，通过模型的最后一层隐藏层输出，
  提取[CLS] token对应的向量作为文本的语义编码向量，确保向量能精准表征文本的上下文语义。

- 第三步，相似度计算与匹配。采用余弦相似度算法，
  计算用户提问编码向量与历史FAQ提问编码向量的相似度值；设置相似度阈值，
  筛选出相似度高于阈值的历史提问，取相似度最高的一条，关联其对应的FAQ回答。

- 第四步，优化与更新。实时监听FAQ录入、编辑、失效操作，
  当有新FAQ新增或旧FAQ修改时，自动对其提问进行编码并更新向量库。
  定期统计匹配准确率，调整相似度阈值与BERT模型参数，确保匹配效果适配业务需求。



